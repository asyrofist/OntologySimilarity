{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Image\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from IPython.display import SVG\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, concatenate, Reshape, Embedding\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['channel', 'lining', 'vastus lateralis']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_fl_nm = '/Users/jaydeep/jaydeep_workstation/Workplace/Java/ontosim/source.json'\n",
    "with open(source_fl_nm) as f:\n",
    "    source_data = json.load(f)\n",
    "    \n",
    "target_fl_nm = '/Users/jaydeep/jaydeep_workstation/Workplace/Java/ontosim/target.json'\n",
    "with open(target_fl_nm) as f:\n",
    "    target_data = json.load(f)\n",
    "\n",
    "onto_dict = [] \n",
    "\n",
    "for key in source_data.keys():\n",
    "    words = source_data[key]['lbl'].split(\"_\")\n",
    "    for word in words:\n",
    "        onto_dict.append(word.lower()) #Converting every word to lower\n",
    "\n",
    "        \n",
    "for key in target_data.keys():\n",
    "    words = target_data[key]['lbl'].split(\"_\")\n",
    "    for word in words:\n",
    "        onto_dict.append(word.lower()) #Converting every word to lower\n",
    "\n",
    "#Create unique dictionary\n",
    "onto_dict = list(set(onto_dict))\n",
    "onto_dict[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[299, 1495, 198]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_embedding_vocab_size = len(onto_dict)\n",
    "encoded_onto_entity = [one_hot(d, onto_embedding_vocab_size)[0] for d in onto_dict]\n",
    "encoded_onto_entity[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to PAD because we are trying to find embedding of words, not sentences.\n",
    "# maxlen = 5\n",
    "# padded_docs_oe = pad_sequences(encoded_docs_oe, maxlen=maxlen, padding='post')\n",
    "# print(padded_docs_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_net():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=onto_embedding_vocab_size,\n",
    "                        output_dim=32, \n",
    "                        input_length=1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04561409,  0.01607234, -0.0280853 , -0.04301577,  0.03786315,\n",
       "       -0.02747009,  0.00582824,  0.01840984,  0.02709312, -0.01771881,\n",
       "       -0.01351823,  0.02070579,  0.02625401,  0.03583862,  0.02028886,\n",
       "        0.02875984, -0.02643342,  0.04025019, -0.04194695, -0.02462727,\n",
       "        0.03833659, -0.04707463,  0.01439152,  0.03361202,  0.04496438,\n",
       "       -0.04145938,  0.01658559, -0.03317313,  0.03591749, -0.04484371,\n",
       "        0.03970841, -0.02806826], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = embed_net()\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(encoded_onto_entity)\n",
    "output_array[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_dict_vec = {}\n",
    "for idx, val in enumerate(onto_dict):\n",
    "    onto_dict_vec[val] = output_array[idx][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in source_data.keys():\n",
    "    words = source_data[key]['lbl'].split(\"_\")\n",
    "    embed_vec = [0.0] * 32\n",
    "    for word in words:\n",
    "        embed_vec = embed_vec + onto_dict_vec[word.lower()]\n",
    "    source_data[key]['vector'] = embed_vec.tolist()\n",
    "    \n",
    "for key in target_data.keys():\n",
    "    words = target_data[key]['lbl'].split(\"_\")\n",
    "    embed_vec = [0.0] * 32\n",
    "    for word in words:\n",
    "        embed_vec = embed_vec + onto_dict_vec[word.lower()]\n",
    "    target_data[key]['vector'] = embed_vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('source_data.json', 'w') as outfile:\n",
    "    json.dump(source_data, outfile, indent=4)\n",
    "    \n",
    "with open('target_data.json', 'w') as outfile:\n",
    "    json.dump(target_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
